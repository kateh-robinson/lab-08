---
title: "Lab 08 - Predicting rain"
author: "Kate Robinson"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidymodels)
```

```{r}
weather <- read.csv("data/weatherAUS.csv", header = TRUE)
```

# Exercise 1: Exploratory Data Analysis

We will start by transform any character variables that need to be transformed into categorical. Use the following code to identify them and transform them into factors.

```{r mutate-char}
variables_to_transform = weather %>% 
  select(where(is.character),-Date) %>% names()
weather <- weather %>% 
  mutate_at(vars(all_of(variables_to_transform)),factor)
```

To simplify things, today we will not be using some categorical explanatory variables, because they have a very large number of categories and might make our model interpretation more complex. Specifically we will exclude `WindGustDir`, `WindDir9am` and `WindDir3pm`. 

```{r remove-wind}
weather <- weather %>%
  select(-WindGustDir,-WindDir9am,-WindDir3pm)
```

Note that some of these variables have a large number of missing values:

```{r find-prop-na}
weather %>% 
  select(where(~ any(is.na(.)))) %>% 
  summarise(across(everything(), ~mean(is.na(.)))) %>%
  pivot_longer(col = everything(), names_to = "Variable", values_to = "Prop_NA") %>%
  arrange(desc(Prop_NA))
```

1. Are there any missing values in our variable of interest `RainTomorrow`? If so, we filter them out and save the new dataset as `weather_noNA`. 

```{r}
#Yes - 0.0225 proportion of NAs
weather_noNA <- weather %>%
  filter(!is.na(RainTomorrow))
```

2. Which cities are the ones with more rain days? To do this, let's analyze the `RainToday` variable. 

```{r}
weather_noNA %>%
  filter(RainToday == "Yes") %>%
  count(Location) %>%
  arrange(desc(n))
```

# Exercise 2: Logistic regression

We will focus our analysis on the city of `Portland`.
Remove `eval=FALSE` from below once you have defined the object `weather_noNA`.

```{r}
# remove eval=FALSE
weather_Portland <- weather_noNA %>%
  filter(Location == "Portland")
```

1. Try to predict `RainTomorrow` by fitting a linear regression using the variable `RainToday` and print the output using `tidy()`.

```{r}
rain_fit <- logistic_reg() %>%
  set_engine("glm") %>%
  fit(RainToday ~ RainTomorrow, data = weather_Portland, family = "binomial")

tidy(rain_fit)
```

2. For each point in our dataset, what are the fitted probabilities that tomorrow it's going to rain? 

- Plot them using an appropriate visualization. What is peculiar about them?

```{r}
rain_pred <- predict(rain_fit, weather_Portland, type = "prob") %>% bind_cols(weather_Portland %>% select(RainToday, Date))

rain_pred %>%
  roc_curve(
    truth = RainToday,
    .pred_Yes,
    event_level = "second"
  ) %>%
  autoplot()
```

> Hint: how many unique values do the predicted probabilities take? What do these value correspond to?

*Only two unique values taken by predicted probabilities - this is shown in graph as only one point off curve.*

- Are there any missing values? Why?

```{r}
rain_pred %>%
  count(is.na(RainToday))

```

*Yes as these occurred in dataset and not removed*

# Exercise 3: Split the data and build workflows

Let us set a seed and perform a split of our data.

```{r seed}
set.seed(111723)
```

1. Split the data into a training set (80% of your Portland data) and a testing set.

```{r}
portland_split <- initial_split(weather_Portland, prop=0.8)
train_data <- training(portland_split)
test_data <- testing(portland_split)
```

2. Refit the simple logistic regression using `RainToday` as predictor on this training data, using `tidymodels` recipes and workflows.

- Start by the recipe. First initialize the recipe, then remove observations with missing values using `step_naomit()` and finally use `step_dummy` to convert categorical to dummy variables.

```{r recipe1}
# remove eval=FALSE
weather_rec1 <- recipe(
  RainTomorrow ~ RainToday,
  data = weather_Portland
  ) %>%
  step_naomit(all_predictors()) %>%
  step_dummy(all_nominal(), -all_outcomes())
```

- Build your workflow combining model and recipe

```{r workflow1}
# remove eval=FALSE
weather_mod1 <- logistic_reg() %>% 
  set_engine("glm")
weather_wflow1 <- workflow() %>% # initiate workflow
  add_model(weather_mod1) %>%                   # add model
  add_recipe(weather_rec1)                       # add recipe
```

- Now fit your model on the training data

```{r fit1}
# remove eval=FALSE
weather_fit1 <- weather_wflow1 %>% 
  fit(data = train_data)
tidy(weather_fit1)
```

3. Fit now a multiple logistic regression, i.e. using multiple explanatory variables, to predict 
`RainTomorrow`. We will use as predictors the variables `MinTemp`, `MaxTemp`, `RainToday` and `Rainfall`. Similarly to question 2, use workflows to fit this model on our training data. 

- Start by the recipe. This will be a simple recipe, because we have not done many pre-processing steps, but do remove missing values and transform categorical variables into dummy variables:

```{r recipe2}
# remove eval=FALSE
weather_rec2 <- recipe(
  RainTomorrow ~ MinTemp+MaxTemp+RainToday+Rainfall, # include your formula
  data = weather_Portland
  ) %>%
  step_naomit(all_predictors()) %>%          # exclude cases with missing values in all predictors
  step_dummy(all_nominal(), -all_outcomes()) # exclude all outcomes
```

- Save the model, workflow and finally, let's fit to the training data.

```{r}
weather_mod2 <- logistic_reg() %>%
  set_engine("glm")
weather_wflow2 <- workflow() %>%
  add_model(weather_mod2) %>%
  add_recipe(weather_rec2)
weather_fit2 <- weather_wflow2 %>%
  fit(data = train_data)
```

3. Now let's evaluate the predictive performance of these two models on our test set.

- Create the ROC curve and get the AUC (area under the curve) value for your first simple logistic regression model.

```{r}
# remove eval=FALSE
weather_pred1 <- predict(weather_fit1, test_data, type = "prob") %>%
  bind_cols(test_data)
weather_pred1 %>%
  roc_curve(                      # plot ROC curve
    truth = RainTomorrow,
    .pred_Yes,
    event_level = "second"
  ) %>%
  autoplot()

weather_pred1 %>%
  roc_auc(                  # get AUC value
    truth = RainTomorrow,
    .pred_Yes,
    event_level = "second"
  )
```

- Create now the ROC curve and get the AUC (area under the curve) value for your second model.

```{r}
weather_pred2 <- predict(weather_fit2, test_data, type = "prob") %>%
  bind_cols(test_data)
weather_pred2 %>%
  roc_curve(                      # plot ROC curve
    truth = RainTomorrow,
    .pred_Yes,
    event_level = "second"
  ) %>%
  autoplot()

weather_pred2 %>%
  roc_auc(                  # get AUC value
    truth = RainTomorrow,
    .pred_Yes,
    event_level = "second"
  )
```

- Which model seems to have a better performance?

*The second model has higher auc value and curve is better shaped.*

4. Now focus on the second model. Consider several thresholds for predicting `RainTomorrow` and look at the number of false positives and false negatives. For example:

```{r eval = FALSE}
# remove eval = FALSE
cutoff_prob <- 0.3
weather_pred2 %>%
  mutate(
    RainTomorrow      = if_else(RainTomorrow == "Yes", "It rains", "It does not rain"),
    RainTomorrow_pred = if_else(.pred_Yes > cutoff_prob, "Predicted rain", "Predicted no rain")
    ) %>%
  na.omit() %>%
  count(RainTomorrow_pred, RainTomorrow)
```

- What is the the false positive rate with `cutoff_prob = 0.3`? 

*0.292 rate*

- What about the false negative rate?

*0.0874 rate*

# Exercise 4: Extend our model [OPTIONAL]

We will now try to improve our fit by building a model using additional explanatory variables.

1. Let us analyze the various variables in our dataset.

- Is there any categorical variable which is very unbalanced? If so, remove it.

- Is there any numerical variable that has a very small standard deviation for one of the two categories of `RainTomorrow`? If so, remove it.

2. Let's do some feature engineering: let us transform the variable `Date`. We will use `Ludbridate` again: extract the month and year.

3. Let's now combine everything into recipes and workflows. Then fit the model on the training data and use the test data for calculating the AUC and plotting the ROC curve.

4. Is this model better than the one we fitted in Exercise 3?
